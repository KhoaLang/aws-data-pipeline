from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator

from datetime import datetime, timedelta
from utils import _local_to_s3, _attach_datetime

# Get exported variables from Airflow
BUCKET_NAME = Variable.get("BUCKET_NAME")

# Default arguments for defining the DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": True,
    "wait_for_downstream": True,
    "start_date": datetime.now(),
    "email": ["airflow@airflow.com"],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=1),
}

# Running DAG everyday at midnight UTC
with DAG(
    "malware_detection",
    default_args=default_args,
    schedule_interval="0 0 * * *",
    max_active_runs=1,
) as dag:

    # On Mondays, attach randomly generated timestamps to each row of the file
    attach_timestamp_to_file = PythonOperator(
        task_id="attach_timestamp_to_file",
        python_callable=_attach_datetime,
        op_kwargs={
            "filename": "/source-data/sample.csv",
            "destination": "/source-data/dated_sample.csv",
        },
    )

    # copies postgres-source data into CSV file
    extract_malware_files_data = PostgresOperator(
        task_id="extract_malware_files_data",
        sql="./sql/extract_malware_files.sql",
        postgres_conn_id="postgres_source",
        params={"malware_files": "/source-data/temp/malware_files.csv"},
        depends_on_past=True,
        wait_for_downstream=True,
    )

    # moves flat file from local system to S3 `raw` folder
    load_malware_files_to_s3_raw = PythonOperator(
        task_id="load_malware_files_to_s3_raw",
        python_callable=_local_to_s3,
        op_kwargs={
            "filename": "/source-data/temp/malware_files.csv",
            "key": "raw/malware_file_detection/{{ ds }}/malware_files.csv",
            "bucket_name": BUCKET_NAME,
            "remove_local": True,
        },
    )

    end_of_data_pipeline = DummyOperator(task_id="end_of_data_pipeline")

    (
        attach_timestamp_to_file
        >> extract_malware_files_data
        >> load_malware_files_to_s3_raw
        >> end_of_data_pipeline
    )
