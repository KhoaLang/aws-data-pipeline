import os
import json
from airflow import DAG
from airflow.models import Variable
from airflow.operators.python import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
from airflow.operators.postgres_operator import PostgresOperator
from airflow.providers.amazon.aws.sensors.emr_job_flow import EmrJobFlowSensor
from airflow.providers.amazon.aws.operators.emr_create_job_flow import EmrCreateJobFlowOperator

from datetime import datetime, timedelta
from utils import _attach_datetime, _local_file_to_s3

# Get exported variables from Airflow
BUCKET_NAME = Variable.get("BUCKET_NAME")

# Provide JOB_FLOW_OVERRIDES argument (JSON for EMR cluster creation)
with open(os.path.join(os.getcwd(), "dags/scripts/emr/create_emr.json"), "r") as f:
    JOB_FLOW_OVERRIDES = json.load(f)

# Specify arguments for Spark script - which script to read, where to read data from,
# and where to write data to. These are the sys.argv arguments in the Spark script
today = datetime.utcnow().date()
S3_URI_SPARK_SCRIPT = (
    "s3://malware-detection-bucket/scripts/spark/malware_file_detection.py"
)
S3_URI_RAW_FILE = f"s3://malware-detection-bucket/raw/malware_file_detection/{today}/malware_files.csv"
S3_URI_STAGE = f"s3://malware-detection-bucket/stage/malware_file_detection/{today}/"
JOB_FLOW_OVERRIDES["Steps"][0]["HadoopJarStep"]["Args"] += [
    S3_URI_SPARK_SCRIPT,
    S3_URI_RAW_FILE,
    S3_URI_STAGE,
]

# Default arguments for defining the DAG
default_args = {
    "owner": "airflow",
    "depends_on_past": True,
    "wait_for_downstream": True,
    "start_date": datetime.now(),
    "email": ["airflow@airflow.com"],
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=1),
}

# Running DAG everyday at midnight UTC
with DAG(
    "malware_detection",
    default_args=default_args,
    schedule_interval="0 0 * * *",
    max_active_runs=1,
) as dag:

    # On Mondays, attach randomly generated timestamps to each row of the file
    attach_timestamp_to_file = PythonOperator(
        task_id="attach_timestamp_to_file",
        python_callable=_attach_datetime,
        op_kwargs={
            "filename": "/source-data/sample.csv",
            "destination": "/source-data/dated_sample.csv",
        },
    )

    # copies postgres-source data into CSV file
    extract_malware_files_data = PostgresOperator(
        task_id="extract_malware_files_data",
        sql="./scripts/sql/extract_malware_files.sql",
        postgres_conn_id="postgres_source",
        params={"malware_files": "/source-data/temp/malware_files.csv"},
        depends_on_past=True,
        wait_for_downstream=True,
    )

    # moves flat file from local system to S3 `raw` folder
    load_malware_files_to_s3_raw = PythonOperator(
        task_id="load_malware_files_to_s3_raw",
        python_callable=_local_file_to_s3,
        op_kwargs={
            "filename": "/source-data/temp/malware_files.csv",
            "key": "raw/malware_file_detection/{{ ds }}/malware_files.csv",
            "bucket_name": BUCKET_NAME,
            "remove_local": False,
        },
    )

    # uploads Python code to S3 `scripts` folder
    upload_code_to_s3_scripts = PythonOperator(
        task_id="upload_code_to_s3_scripts",
        python_callable=_local_file_to_s3,
        op_kwargs={
            "filename": "./dags/scripts/spark/malware_file_detection.py",
            "key": "scripts/spark/malware_file_detection.py",
            "bucket_name": BUCKET_NAME,
            "remove_local": False,
        },
    )

    # creates an EMR cluster according to the settings in JOB_FLOW_OVERRIDES
    # and then runs the specified Spark step
    # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/emr.html#EMR.Client.run_job_flow
    create_and_run_emr_cluster = EmrCreateJobFlowOperator(
        task_id="create_and_run_emr_cluster",
        job_flow_overrides=JOB_FLOW_OVERRIDES,
        aws_conn_id="aws_default",
        emr_conn_id="emr_default",
    )

    check_emr_step_completion = EmrJobFlowSensor(
        task_id="check_emr_step_completion",
        job_flow_id=create_and_run_emr_cluster.output,
    )

    # start_redshift_cluster

    end_of_data_pipeline = DummyOperator(task_id="end_of_data_pipeline")

    (
        attach_timestamp_to_file
        >> extract_malware_files_data
        >> [
            load_malware_files_to_s3_raw,
            upload_code_to_s3_scripts,
        ]
        >> create_and_run_emr_cluster
        >> check_emr_step_completion
        >> end_of_data_pipeline
    )
